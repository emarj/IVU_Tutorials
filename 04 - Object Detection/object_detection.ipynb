{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f752819",
   "metadata": {},
   "source": [
    "# Object Detection & Co.\n",
    "\n",
    "\n",
    "Download data from: https://drive.google.com/file/d/1JNnpaIf8fnlo5vum-D5ZpeRVFLyPIz-z/view?usp=sharing\n",
    "\n",
    "![](images/tasks.png)\n",
    "\n",
    "## Basic Idea\n",
    "\n",
    "![](images/arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf34bb3",
   "metadata": {},
   "source": [
    "\n",
    "## YOLO (**You Only <s>Live</s> Look Once**)\n",
    "\n",
    "YOLO is a series of very popular **real-time** object detection systems with very good performance.\n",
    "\n",
    "![](./images/yolo-arch.png)\n",
    "\n",
    "## (Partial) History of YOLO algorithm\n",
    "\n",
    "| Version           | Year | Authors / Organization                    | New Publication or Refinement | Notes                                                                       |                                                                                                               |\n",
    "| ----------------- | ---- | ----------------------------------------- | ----------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |\n",
    "| YOLOv1            | 2016 | Joseph Redmon et al.                      | New Publication               | Introduced unified real-time object detection.                              |                                                                                                               |\n",
    "| YOLOv2 (YOLO9000) | 2017 | Joseph Redmon, Ali Farhadi                | New Publication               | Improved accuracy and speed; trained jointly on classification + detection. |                                                                                                               |\n",
    "| YOLOv3            | 2018 | Joseph Redmon, Ali Farhadi                | New Publication               | Better performance with multi-scale predictions and deeper network.         |                                                                                                               |\n",
    "| YOLOv4            | 2020 | Alexey Bochkovskiy et al.                 | Refinement                    | Based on Darknet; used bag-of-freebies and bag-of-specials techniques.      |                                                                                                               |\n",
    "| YOLOv5            | 2020 | Ultralytics                               | Refinement (Unofficial fork)  | Not published in peer-reviewed form; rewritten in PyTorch.                  |                                                                                                               |\n",
    "| YOLOv6            | 2022 | Meituan (open-sourced by them)            | New Publication               | Focused on industrial applications, speed-accuracy trade-off.               |                                                                                                               |\n",
    "| YOLOv7            | 2022 | Chien-Yao Wang et al. (Wong Kin Yiu team) | New Publication               | Introduced new training methods and model architectures.                    |                                                                                                               |\n",
    "| YOLOv8            | 2023 | Ultralytics                               | Refinement                    | Not a formal publication; introduced improved PyTorch implementation.       |                                                                                                               |\n",
    "| YOLOv9            | 2024 | Chien-Yao Wang et al. (CVPR 2024 paper)   | New Publication               | Introduced Generalized ELAN and DyHead; state-of-the-art performance.       |                                                                                                               |\n",
    "| YOLOv10           | 2024 | Ao Wang et al. (NeurIPS 2024 paper)       | New Publication               | Proposed NMS-free training and holistic efficiency-accuracy design.         |                                                                                                               |\n",
    "| YOLOv11           | 2024 | Rahima Khanam, Muhammad Hussain           | New Publication               | Introduced C3k2, SPPF, and C2PSA components; expanded capabilities.         |                                                                                                               |\n",
    "| YOLOv12           | 2025 | Yunjie Tian, Qixiang Ye, David Doermann   | New Publication               | Attention-centric design with competitive speed and accuracy.               |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ultralytics YOLO11\n",
    "\n",
    "We will use the very popular library Ultralytics due to its easy of use. It is important to know that there is some controversy about the licensing, read [this](https://www.reddit.com/r/computervision/comments/1e3uxro/ultralytics_new_agpl30_license_exploiting/) if interested. Before using it for commercial projects or for publications you should definitely read the license.\n",
    "\n",
    "Have a look at the [documentation](https://docs.ultralytics.com/usage/python/) to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q ultralytics opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def show_img(input,convert_to_rgb=True):\n",
    "    # Internally ultralitics works with OpenCV so we need to convert to RGB\n",
    "    if convert_to_rgb:\n",
    "        input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = Image.fromarray(input)\n",
    "\n",
    "    display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23643e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('models/yolo11n.pt')\n",
    "\n",
    "results = model('./data/pedestrian.jpg')\n",
    "\n",
    "# otherwise\n",
    "# results = model.predict(source='./data/pedestrian.jpg')\n",
    "\n",
    "print(f'Found {len(results)} results')\n",
    "\n",
    "# Process results list\n",
    "\n",
    "for result in results:\n",
    "    #print(result.names)\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    #print(probs)\n",
    "\n",
    "    # This is always present\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    #print('Boxes')\n",
    "    #print(boxes)\n",
    "\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    #print('Masks')\n",
    "    #print(masks)\n",
    "\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    #print('Keypoints')\n",
    "    #print(keypoints)\n",
    "\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    #print('OBB')\n",
    "    #print(obb)\n",
    "\n",
    "    img = result.plot() # plot to img object\n",
    "    show_img(img)\n",
    "\n",
    "    #result.show()  # display to screen\n",
    "    #result.save(filename=\"result.jpg\")  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2model_name(task):\n",
    "    if task:\n",
    "        task = '-' + task\n",
    "    else:\n",
    "        task = ''\n",
    "        \n",
    "    return f'models/yolo11n{task}.pt'\n",
    "\n",
    "\n",
    "def yolo_img(input, task=None):\n",
    "    \n",
    "    model_name = task2model_name(task)\n",
    "    print(model_name)\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    res = model(input)\n",
    "\n",
    "    show_img(res[0].plot())\n",
    "\n",
    "tasks = [None,'seg','pose','obb']\n",
    "\n",
    "for task in tasks:\n",
    "    \n",
    "    res = yolo_img('./data/pedestrian.jpg', task=task)\n",
    "    \n",
    "\n",
    "\n",
    "# yolo_img('./data/port.png', task='obb')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8538bd1f",
   "metadata": {},
   "source": [
    "#### Exercise: YOLO Fine-tuning\n",
    "- Use YOLO-seg to do tumor segmentation on BrainMRI dataset we used for image segmentation. Since YOLO-seg expects also the bounding box you need to generate bounding boxes from segmentation masks.\n",
    "- Add more classes to YOLO (for example pencil/pen)\n",
    "\n",
    "See ultralytics documentations on how to organize the dataset and how to train the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486ba0a",
   "metadata": {},
   "source": [
    "### Working with videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b27f46",
   "metadata": {},
   "source": [
    "`ultralytics` directly support inference on video data, but we will do it ourselves using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c647cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"./data/dancing_1.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"./data/tennis_1.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92431190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def yolo_video(task, input, output=None,show=True, device='cpu'):\n",
    "\n",
    "    model_name = task2model_name(task)\n",
    "    print(model_name)\n",
    "    model = YOLO(model_name)\n",
    "    \n",
    "    cap = cv2.VideoCapture(input)\n",
    "\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    if output:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "   \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Run YOLO inference\n",
    "        results = model.predict(source=frame, stream=True, task=task, conf=0.5, device=device)\n",
    "    \n",
    "        #annotated = results[0].plot() # this for streaming=False\n",
    "        for r in results:\n",
    "            annotated = r.plot()  # Returns annotated frame\n",
    "\n",
    "        # Convert BGR to RGB for display\n",
    "        rgb_frame = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(rgb_frame)\n",
    "\n",
    "        # Display in notebook\n",
    "        clear_output(wait=True)\n",
    "        display(img)\n",
    "        \n",
    "        if output:\n",
    "            out.write(annotated)\n",
    "\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    if output:\n",
    "        out.release()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_video('seg',\"./data/tennis_1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_video('pose',\"./data/tennis_1.mp4\",\"./results/tennis_out.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c201e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_video('pose',\"./data/dancing_1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_video('pose',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801cf197",
   "metadata": {},
   "source": [
    "#### Copyright Disclaimer\n",
    "\n",
    "This notebook uses parts of the following videos:\n",
    "- [The Ten Rules of Techno (performance)](https://www.youtube.com/watch?v=jm-rMP9bvbU) by **Underdog Electronic Music School** \n",
    "- [Practicing With 12 UT**R - How Do College Tennis Players Train?](https://www.youtube.com/watch?v=HTpcVloOwbQ) by **MPTennis**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
