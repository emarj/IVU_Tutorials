{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7d05a",
   "metadata": {},
   "source": [
    "### Define dataset class\n",
    "\n",
    "\n",
    "Dataset downloaded from https://www.kaggle.com/datasets/mateuszbuda/lgg-mri-segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23418406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from natsort import natsorted\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class BrainMRIDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 transform = None,\n",
    "                 target_transform = None,\n",
    "                ):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.image_mask_pairs = []\n",
    "        \n",
    "\n",
    "        # Scan folder structure and add all the images and masks\n",
    "\n",
    "        \n",
    "        patients_dirs = sorted([os.path.join(root_dir, entry) for entry in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, entry))])\n",
    "        self.patients_offset = [0]*len(patients_dirs)\n",
    "\n",
    "        offset = 0\n",
    "\n",
    "        for patient_idx, patient in enumerate(patients_dirs):\n",
    "\n",
    "            self.patients_offset[patient_idx] = offset\n",
    "\n",
    "            images = [os.path.join(patient, entry) for entry in os.listdir(patient)]\n",
    "            images = natsorted([img for img in images if \"_mask\" not in img])\n",
    "\n",
    "            for img_path in images:\n",
    "                    base_name = os.path.splitext(img_path)[0]\n",
    "                    mask_path = base_name + \"_mask.tif\"\n",
    "                    if not os.path.exists(mask_path):\n",
    "                        warnings.warn(f\"Image {img_path} is missing mask, skipping\")\n",
    "                    else:\n",
    "                        self.image_mask_pairs.append((img_path, mask_path))\n",
    "            \n",
    "            offset = len(self.image_mask_pairs) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_mask_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.image_mask_pairs[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def _get_patient_samples_indices(self, idx):\n",
    "        if idx == len(self.patients_offset) - 1:\n",
    "            end = len(self)\n",
    "        else:\n",
    "            end = self.patients_offset[idx+1]\n",
    "        \n",
    "        return list(range(self.patients_offset[idx],end))\n",
    "    \n",
    "    def get_patient_samples(self, idx):\n",
    "        return [self[i] for i in self._get_patient_samples_indices(idx)]\n",
    "    \n",
    "    \n",
    "\n",
    "    def num_patients(self):\n",
    "        return len(self.patients_offset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0096a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "def overlay_mask(image, mask, color=(1.0, 0.0, 0.0), alpha=0.45, saturation=0.2):\n",
    "    \"\"\"\n",
    "    Overlay a binary mask onto an image tensor.\n",
    "\n",
    "    Parameters:\n",
    "        image (torch.Tensor): Float tensor (3, H, W), values in [0, 1]\n",
    "        mask (torch.Tensor): Binary tensor (1, H, W), values in {0, 1}\n",
    "        color (tuple): RGB tuple in [0, 1], e.g., (1, 0, 0) for red\n",
    "        alpha (float): Overlay transparency (0 = transparent, 1 = solid)\n",
    "        saturation (float): factor in [0,1] of saturation adjustment applied to the image (0 = bw, 1 = untouched).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: New image tensor with overlay applied (3, H, W)\n",
    "    \"\"\"\n",
    "    if image.ndim != 3 or image.shape[0] != 3:\n",
    "        raise ValueError(\"Image must be a tensor of shape (3, H, W)\")\n",
    "\n",
    "    if mask.ndim != 3 or mask.shape[0] != 1:\n",
    "        raise ValueError(\"Mask must be a tensor of shape (1, H, W)\")\n",
    "\n",
    "\n",
    "    if image.shape[1:] != mask.shape[1:]:\n",
    "        raise ValueError(f\"Spatial dimensions of mask must match dimensions of the image\")\n",
    "\n",
    "    # Convert image to grayscale if grey_scale is True\n",
    "    if saturation < 1.0:\n",
    "        image = F.adjust_saturation(image,saturation)\n",
    "\n",
    "    if image.max() > 1:\n",
    "        image = image / 255.0\n",
    "\n",
    "    # Create color mask\n",
    "    color_tensor = torch.tensor(color, dtype=image.dtype, device=image.device).view(3, 1, 1)\n",
    "    mask = (mask > 0.5).float()  # Convert mask to binary values (0 or 1)\n",
    "\n",
    "    # Blend color and image where mask == 1\n",
    "    overlay = image * (1 - mask * alpha) + color_tensor * (mask * alpha)\n",
    "    return overlay.clamp(0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e97f5",
   "metadata": {},
   "source": [
    "### Initialize and Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_samples(samples,title=None,saturation=0.2):\n",
    "\n",
    "    samples_img = [overlay_mask(s[0],s[1],saturation=saturation) for s in samples]\n",
    "\n",
    "    img = utils.make_grid(samples_img, min(len(samples_img),8))\n",
    "\n",
    "    plt.figure(figsize = (10,10))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_pair(sample,title=None):\n",
    "    img, mask = sample\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.title('Image')\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "    mask = mask.repeat(3,1,1)\n",
    "    mask[1:] = 0\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Mask')\n",
    "    plt.imshow(mask.permute(1,2,0))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5dcddf",
   "metadata": {},
   "source": [
    "### Split data and define Dataloaders\n",
    "\n",
    "We will use a PyTorch Lightning [DataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). This is not really needed, you can use plain data loaders if you prefer. The nice thing about this is that it encapsulate all the logic that deals with the data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2aca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import lightning as L\n",
    "\n",
    "class BrainMRIDataModule(L.LightningDataModule):\n",
    "    def __init__(self, root_dir, batch_size=64, num_workers=0, split_ratio=(0.8, 0.1, 0.1)):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self.full_set = None\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.split_ratio = split_ratio\n",
    "\n",
    "        if not torch.isclose(torch.tensor(sum(split_ratio)), torch.tensor(1.0)):\n",
    "            raise ValueError(f\"Split ratios must sum to 1.0, got sum={sum(split_ratio)}\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Converts image to tensor in [0, 1]\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.Grayscale(3), # Optional, experiment with and without!\n",
    "        ])\n",
    "\n",
    "        self.full_set = BrainMRIDataset(self.root_dir,transform=transform,target_transform=transform)\n",
    "        \n",
    "        dataset_len = len(self.full_set)\n",
    "\n",
    "        train_len = int(self.split_ratio[0] * dataset_len)\n",
    "        val_len = int(self.split_ratio[1] * dataset_len)\n",
    "        test_len = dataset_len - train_len - val_len\n",
    "\n",
    "        self.train_set, self.val_set, self.test_set = random_split(\n",
    "            self.full_set, [train_len, val_len, test_len]\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dm = BrainMRIDataModule('./data/lgg-mri-segmentation/kaggle_3m')\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "ds = dm.full_set\n",
    "\n",
    "# show a pair (img,mask)\n",
    "idx = 10\n",
    "show_pair(ds[idx],f'Sample {idx}')\n",
    "\n",
    "# show samples for a patient\n",
    "patient_idx = 10\n",
    "show_samples(ds.get_patient_samples(patient_idx),f'Patient {patient_idx}')\n",
    "\n",
    "# show first N samples\n",
    "n_rand_samples = 80\n",
    "show_samples([ds[i] for i in torch.randperm(n_rand_samples)],f'Random {n_rand_samples} samples',saturation=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936857e",
   "metadata": {},
   "source": [
    "## Define the Loss and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bafa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet segmentation_models_pytorch\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad607a62",
   "metadata": {},
   "source": [
    "### Define the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad4042",
   "metadata": {},
   "source": [
    "Several metrics are useful in evaluating a segmentation model. Since a segmentation problem can be seen as a pixel-wise classification, `CrossEntropyLoss` (or in this case `BinaryCrossEntropyLoss`) can be used.\n",
    "Another approach is to use geometric methods that measure, in some way, the overlap of the areas.\n",
    "\n",
    "![](./images/dicevsiou.png)\n",
    "\n",
    "\n",
    "Quick schema on which one to pick\n",
    "\n",
    "| Scenario                        | Recommended Loss                     |\n",
    "|----------------------------------|---------------------------------------|\n",
    "| Balanced dataset                | CrossEntropy                          |\n",
    "| Class imbalance (e.g., organs)  | Dice Loss         |\n",
    "| Strict overlap quality needed   | IoU or Combo: Dice + IoU              |\n",
    "| Multi-class segmentation        | CE or per-class Dice averaged         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ecee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationLoss(torch.nn.Module):\n",
    "    def __init__(self,alpha=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.bce_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss_fn = smp.losses.DiceLoss(mode='binary')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        loss = self.alpha*self.bce_loss_fn(logits,targets) + (1-self.alpha)*self.dice_loss_fn(logits,targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eafa9bc",
   "metadata": {},
   "source": [
    "**Exercise** Experiment further with these losses trying to tune the weights `alpha`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8429e6",
   "metadata": {},
   "source": [
    "### Construct the model\n",
    "\n",
    "#### ResNet34\n",
    "\n",
    "![ResNet34](./images/resnet34.jpg)\n",
    "\n",
    "\n",
    "#### U-Net\n",
    "\n",
    "![](./images/u-net-original.png)\n",
    "\n",
    "![](./images/unet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = ConvBlock(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        base_model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        self.input_conv = nn.Sequential(\n",
    "            base_model.conv1, base_model.bn1, base_model.relu\n",
    "        )\n",
    "        self.maxpool = base_model.maxpool\n",
    "        self.encoder1 = base_model.layer1  # 64\n",
    "        self.encoder2 = base_model.layer2  # 128\n",
    "        self.encoder3 = base_model.layer3  # 256\n",
    "        self.encoder4 = base_model.layer4  # 512\n",
    "\n",
    "        self.center = ConvBlock(512, 512)\n",
    "\n",
    "        self.decoder4 = DecoderBlock(512, 256, 256)\n",
    "        self.decoder3 = DecoderBlock(256, 128, 128)\n",
    "        self.decoder2 = DecoderBlock(128, 64, 64)\n",
    "        self.decoder1 = DecoderBlock(64, 64, 32)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.final_conv = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x0 = self.input_conv(x)        # H/2\n",
    "        x1 = self.maxpool(x0)          # H/4\n",
    "\n",
    "        x2 = self.encoder1(x1)         # H/4\n",
    "        x3 = self.encoder2(x2)         # H/8\n",
    "        x4 = self.encoder3(x3)         # H/16\n",
    "        x5 = self.encoder4(x4)         # H/32\n",
    "\n",
    "        center = self.center(x5)\n",
    "\n",
    "        d4 = self.decoder4(center, x4)\n",
    "        d3 = self.decoder3(d4, x3)\n",
    "        d2 = self.decoder2(d3, x2)\n",
    "        d1 = self.decoder1(d2, x0)\n",
    "        \n",
    "        up = self.upsample(d1)\n",
    "        out = self.final_conv(up)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e842d",
   "metadata": {},
   "source": [
    "**Exercise** It is possible to substitute the transposed convolution operation with upsampling followed by a \"normal\" convolution. In this case the \"upscaling\" part is not learned but fixed. Try to experiment with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df05401",
   "metadata": {},
   "source": [
    "### Define the Lightning Module (train/val/test steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00747327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModule(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # saves arguments passed to the constructor\n",
    "\n",
    "        self.model = ResNetUNet(n_classes=1)\n",
    "        # to load predefined smp.Unet\n",
    "        #self.model = smp.Unet(encoder_name=\"resnet34\", in_channels=3, classes=1, encoder_weights='imagenet')\n",
    "        self.loss_fn = SegmentationLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = self.loss_fn(y_hat,y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = self.loss_fn(y_hat,y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = self.loss_fn(y_hat,y)\n",
    "\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8848e",
   "metadata": {},
   "source": [
    "### Train it and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b522b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model = SegmentationModule()\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=\"./logs\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    #fast_dev_run=True, # activate this when you just want to see if the code runs\n",
    "    min_epochs=5,\n",
    "    max_epochs=15,\n",
    "    accelerator=\"mps\",\n",
    "    log_every_n_steps=5,\n",
    "    #precision='16-mixed', # if your hardware supports it, you can use a different precision\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor='val_loss'),\n",
    "        EarlyStopping(monitor='val_loss'),\n",
    "        ]\n",
    ")\n",
    "\n",
    "trainer.fit(model,datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b98985",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading model checkpoint from {trainer.checkpoint_callback.best_model_path}')\n",
    "model = SegmentationModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52980eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model,datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1de0e",
   "metadata": {},
   "source": [
    "### Visualize some predictions\n",
    "\n",
    "This code is a bit messy. Improve it by using dataloader and batches of data (you should also update the `show_samples` function to support batches of dat!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    ds = dm.test_set\n",
    "\n",
    "    print(len(ds))\n",
    "\n",
    "    # show a pair (img,mask)\n",
    "    samples = [ds[i] for i in range(20)]\n",
    "    show_samples(samples,'GT')\n",
    "\n",
    "    ys_hat = [model(s[0].unsqueeze(0)).squeeze(0) for s in samples]\n",
    "\n",
    "    preds = [(torch.sigmoid(y_hat) > 0.5).float() for y_hat in ys_hat]\n",
    "\n",
    "\n",
    "    show_samples(list(zip([s[0] for s in samples],preds)),'Predictions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b037b",
   "metadata": {},
   "source": [
    "**Exercise**. Compute the number of FN (False Negatives), samples in which the model was not able to find a tumor that was present. Technically the impact of this FN is not that much since for every patient we have several images to analyze. This analysis should be done at the patient level and not at the image level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
